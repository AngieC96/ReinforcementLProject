{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdspU5QVN7BQ"
   },
   "source": [
    "# Breakout training with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpSZJAb1uNUd"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hgE0d2CsRVEp"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os                                      # to create folders\n",
    "import gym                                     # contains the game environment\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from datetime import datetime                  # to print a timestamp\n",
    "import pickle                                  # to save on file\n",
    "import torch                                   # ANNs\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IJB_FAOiKuV5"
   },
   "outputs": [],
   "source": [
    "# Tracking version for saving weights\n",
    "version = \"03\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nPvkGXSkKuV7"
   },
   "outputs": [],
   "source": [
    "# Creates the game environment\n",
    "\n",
    "#env = gym.make('Breakout-v0').unwrapped\n",
    "env = gym.make('BreakoutDeterministic-v4').unwrapped\n",
    "#env = gym.make('BreakoutNoFrameskip-v4').unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaLGz1W2uV0h"
   },
   "source": [
    "## Set Up Device\n",
    "\n",
    "We import IPython's display module to aid us in plotting images to the screen later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "erns3ihuNtca"
   },
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoH0D_-6uvwl"
   },
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nUUdYuRARtGl"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Initialize a deep Q-learning network\n",
    "    \n",
    "    Hints:\n",
    "    -----\n",
    "        Original paper for DQN\n",
    "    https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, img_height, img_width, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d\n",
    "        def conv2d_size_out(size, kernel_size, stride=1, padding=0):\n",
    "            return int(size + 2 * padding - kernel_size) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(\n",
    "                    conv2d_size_out(\n",
    "                        conv2d_size_out(img_height, kernel_size=8, stride=4\n",
    "                    ), kernel_size=4, stride=2\n",
    "                ), kernel_size=3, stride=1)\n",
    "        convh = conv2d_size_out(\n",
    "                    conv2d_size_out(\n",
    "                        conv2d_size_out(img_width, kernel_size=8, stride=4\n",
    "                    ), kernel_size=4, stride=2\n",
    "                ), kernel_size=3, stride=1)\n",
    "        \n",
    "        linear_input_size = convw * convh * 64  # = 7 * 7 * 64 = 3136\n",
    "        \n",
    "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Calculates probability of each action.\n",
    "        Called with either one element to determine next action, or a batch during optimization.\n",
    "        NOTE: a single discrete state is collection of 4 frames\n",
    "        :param x: processed state of shape b x 4 x 84 x 84\n",
    "        :returns tensor of shape [batch_size, n_actions] (estimated action values)\n",
    "        \"\"\"\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))  # b x 32 x 20 x 20\n",
    "        x = F.relu(self.conv2(x))  # b x 64 x 9 x 9\n",
    "        x = F.relu(self.conv3(x))  # b x 64 x 7 x 7\n",
    "        x = x.view(x.size(0), -1)  # b x (7 * 7 * 64) x 1\n",
    "        x = F.relu(self.fc1(x))    # b x 512\n",
    "        x = self.fc2(x)            # b x  4\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Dndx7phTKuWC"
   },
   "outputs": [],
   "source": [
    "folder_save = \"models\"\n",
    "os.makedirs(folder_save, exist_ok=True)\n",
    "folder_checkp = \"checkpoints_\" + version\n",
    "filename_checkp = os.path.join(folder_save, folder_checkp)\n",
    "os.makedirs(filename_checkp, exist_ok=True)\n",
    "\n",
    "filename_durations = \"durations.pickle\"\n",
    "filename_rewards   = \"rewards.pickle\"\n",
    "filename_losses    = \"losses.pickle\"\n",
    "\n",
    "def exchange_weights(net1, net2):\n",
    "    net1.load_state_dict(net2.state_dict())\n",
    "\n",
    "def load_weights(net, filename):\n",
    "    filename = os.path.join(folder_save, filename)\n",
    "    net.load_state_dict(torch.load(filename))\n",
    "\n",
    "def save_weights(net, filename: str):\n",
    "    filename = os.path.join(folder_save, filename + \".pt\")\n",
    "    torch.save(net.state_dict(), filename)\n",
    "    \n",
    "def save_checkpoint(net, optimizer, episode, tot_steps_done):\n",
    "    checkpoint_dict = {\n",
    "        \"parameters\"    : net.state_dict(),\n",
    "        \"optimizer\"     : optimizer.state_dict(),\n",
    "        \"episode\"       : episode,\n",
    "        \"tot_steps_done\": tot_steps_done\n",
    "    }\n",
    "    filename_checkp = os.path.join(filename_checkp, \"checkpoint_\" + str(episode) +\".pt\")\n",
    "    torch.save(checkpoint_dict, filename)\n",
    "    \n",
    "def save_vectors4plots(episode_durations, episode_rewards, losses):\n",
    "    # save the vectors\n",
    "    filename_durations = os.path.join(filename_checkp, filename_durations)\n",
    "    outfile = open(filename_durations, 'wb')\n",
    "    pickle.dump(episode_durations, outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "    filename_rewards = os.path.join(filename_checkp, filename_rewards)\n",
    "    outfile = open(filename_rewards, 'wb')\n",
    "    pickle.dump(episode_rewards, outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "    filename_losses = os.path.join(filename_checkp, filename_losses)\n",
    "    outfile = open(filename_losses, 'wb')\n",
    "    pickle.dump(losses, outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFmilqPxv6EN"
   },
   "source": [
    "## Experience class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fwgsUELpRuH9"
   },
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TM3kAq15R9WY",
    "outputId": "6002ad1c-8d97-4da2-f682-47356c32ec00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=2, action=3, next_state=1, reward=4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Experience(2,3,1,4)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UshQ99EvKuWH"
   },
   "source": [
    "## StateHolder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sEwxXzh9KuWI"
   },
   "outputs": [],
   "source": [
    "class StateHolder:\n",
    "    \"\"\" Class which stores the state of the game.\n",
    "    We will use 4 consecutive frames of the game stacked together.\n",
    "    This is necessary for the agent to understand the speed and acceleration of game objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, number_screens = 4):\n",
    "        self.first_action = True\n",
    "        self.state = torch.ByteTensor(1, 84, 84).to(device)\n",
    "        self.number_screens = number_screens\n",
    "        \n",
    "    def push(self, screen):\n",
    "        new_screen = screen.squeeze(0) # If the first dimension (dim=0) of the tensor is 1, remove it\n",
    "        if self.first_action:\n",
    "            self.state[0] = new_screen\n",
    "            for number in range(self.number_screens-1):\n",
    "                self.state = torch.cat((self.state, new_screen), 0)\n",
    "            self.first_action = False\n",
    "        else:\n",
    "            self.state = torch.cat((self.state, new_screen), 0)[1:] # append the new frame and remove the first one, so that it will always contain 4 screens\n",
    "    \n",
    "    def get(self):\n",
    "        return self.state.unsqueeze(0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.first_action = True\n",
    "        self.state = torch.ByteTensor(1, 84, 84).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4bIgT98wbcJ"
   },
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UgbpipIRR-IC"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "  \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0       # Number of experiences added to the memory\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(Experience(*args))\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = Experience(*args)  # overwrite the first experiences first\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size    # can we sample from memory?\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEcJ15vIx8Hz"
   },
   "source": [
    "## Epsilon Greedy Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A1vPemxix8gc"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, agent_current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * agent_current_step * self.decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9Ko3XqYyqHd"
   },
   "source": [
    "## Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sqWEsCB1yq0o"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.strategy     = strategy\n",
    "        self.num_actions  = num_actions # number of actions that can be taken from a given state\n",
    "        self.device       = device\n",
    "\n",
    "    def select_action(self, current_step, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(current_step)\n",
    "\n",
    "        if rate > random.random() and state is not None:\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([[action]], device=self.device, dtype=torch.long) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():  # since it's not training\n",
    "                return policy_net(state.float()).argmax(dim=1).to(self.device).view(1, 1) # exploit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKWuHrbQ2Fzl"
   },
   "source": [
    "## Environment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OJ3kpCox2Ghq"
   },
   "outputs": [],
   "source": [
    "STATE_W = 84\n",
    "STATE_H = 84\n",
    "\n",
    "class EnvManager():\n",
    "\n",
    "    def __init__(self, env, device):\n",
    "        self.device = device\n",
    "        self.env = env\n",
    "        self.env.reset() # to have an initial observation of the env\n",
    "        self.max_lives = self.env.ale.lives()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Resets the env to the initial state\n",
    "        \"\"\"\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Closes the env\n",
    "        \"\"\"\n",
    "        self.env.close()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "    def take_action(self, action):        \n",
    "        _, reward, self.done, info = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device), info\n",
    "\n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\" Returns the current state of the env in the form of a procesed image of the screen\n",
    "        \"\"\"\n",
    "        s = self.get_processed_screen()\n",
    "        self.current_screen = s\n",
    "        return s\n",
    "\n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "\n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "\n",
    "    def get_processed_screen(self):\n",
    "        screen = self.render(mode='rgb_array')\n",
    "        screen = np.dot(screen[...,:3], [0.299, 0.587, 0.114]) # transform to gray scale\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "\n",
    "    def crop_screen(self, screen):\n",
    "        # Strip off top and bottom\n",
    "        return screen[32:195,:]\n",
    "\n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to uint, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.uint8).reshape(screen.shape[0],screen.shape[1],1)\n",
    "\n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((STATE_W, STATE_H)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        return resize(screen).mul(255).type(torch.ByteTensor).to(device).detach().unsqueeze(0) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZvTN-tj7AhC"
   },
   "source": [
    "### Example Screens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2GZMPOp7C5w"
   },
   "source": [
    "#### Non-Processed Screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "3LbI50C06_6L",
    "outputId": "952a8dc6-764e-4dd6-f528-6a38cb6f7863"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "em = EnvManager(env, device)\n",
    "em.reset()\n",
    "screen = em.render('rgb_array')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(screen)\n",
    "plt.title('Non-processed screen example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x07pYbGO7LSL"
   },
   "source": [
    "#### Processed Screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "hQRimcXP7NDA",
    "outputId": "f6df5340-1df0-43e4-dcde-47293259b0db"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUtUlEQVR4nO3de5RdZX3G8e/DJBASrslgEgg3NQUCSIgRodAqt3IVXK1SqBdUrNYqgtqloC3KgnaBq1VZtKVNQUGLXBpIRVQiYqgLqeGaWmCCgQC5SEgChJBAaC6//rHfM26GueyZc2bOOXmfz1pnzb6cs/e755zn7He/s+d9FRGY2dZvm2YXwMxGhsNulgmH3SwTDrtZJhx2s0w47GaZcNgzJeluSR9vdjls5DjsvZD0tKRXJa2T9JykayXt0OxymdXDYe/beyJiB2AGMBP4655PkDRqxEvVJiR1NLsMvcn5PXPYBxARy4GfAAcBSApJn5a0CFiUlv25pCckvSDpNkm7114v6UBJd6Z1z0n6clq+jaQLJD0p6XlJN0san9aNkfTvafkaSfdLmpjWfUTSYkkvS3pK0gdK+/qYpC5JL0qaK2nv0rrjJS2U9JKkfwTU1zFLOkzSA5LWpjJ/o7TuKEn3pnItlfSRtPxaSVdJ+rGk9cDRknaXdIukVamsny1tp7/j3yf9ns+WtETSaklf6ae8EyT9MJX3fkmXSrqntL639+yKVP61kh6U9Adp+SRJr0iaUHr9jHQMo/sqQ1uICD96PICngePS9J7Ao8AlaT6AO4HxwPbAMcBqihrAdsCVwC/Sc3cEngW+AIxJ8+9M684DfgVMSa/7V+CGtO6TwA+BsUAH8HZgJ2AcsBbYLz1vMnBgmj4deAI4ABhFURO5N63rBF4G3geMBj4HbAI+3sfx/zfwoTS9A3B4mt47beestJ0JwPS07lrgJeBIipPIWOBB4CJgW+DNwGLghArHv0/6Pf9b+h0fArwGHNBHeW9Mj7HANGApcE9p/eves7Tsg6n8o9L7swIYk9b9GPhU6fXfBK5s9uey7s91swvQio8U9nXAGuAZ4J9LH5IAjik99xrg66X5HYCN6QN7FvBwH/voAo4tzU9OrxsFfAy4F3hbj9eMS2X6k1p5Sut+ApxTmt8GeCUF9MPAr0rrBCzrJ+y/AC4GOnssvxCY08drrgW+W5p/J7Ckl9d/p8Lx18I+pbT+PuDMXvbbkV63X2nZpb2E/Zjeyl16zovAIWn6T4Fflra/Ajis2Z/Leh+uxvftvRGxS0TsHRF/GRGvltYtLU3vTvGFAEBErAOeB/agqBU82cf29wbmpOrwGooP/2ZgIvA9YC5wo6TfSvq6pNERsZ7ig/gXwLOSfiRp/9L2riht7wWKUO+Rythd5ig+xeVj6Okc4PeAhalafGpa3t/x0GObewO718qTyvTldHwDHX/NitL0KxRfpD3tRvEFUd53b8f2umWS/ipd8ryU9r8zRQ0I4AfANEn7AscDL0XEfb0fcvtw2Iem/K+Cv6X44AIgaRxF9XA5xQfszX1sYylwUvpCqT3GRMTyiNgYERdHxDTg94FTKc7ORMTciDie4ky4kKKqW9veJ3tsb/uIuJfiUmLPUhlVnn/DwUUsioizgDcBlwOz03EtBd5S8feyFHiqR3l2jIiTBzr+frbfm1UUlyRTSst6O7busqXr8y8CZwC7RsQuFJcgAoiIDcDNFFX9D1F8+bY9h71+NwAflTRd0nbA3wHzI+Jp4HZgsqTzJW0naUdJ70yv+xfgb2uNaJJ2k3R6mj5a0sEqWrTXUlRTt0iaKOn0FLzXKC41tpS2d6GkA9M2dpb0/rTuR8CBkv5YRWv0Z4FJfR2QpA9K2i0itlBcNpD2cz1wnKQzJI1KDWPT+9jMfcDLkr4kaXtJHZIOkvSOgY5/MCJiM3Ar8DVJY1NN58MDvGxHii+IVcAoSRdRtImUfRf4CHAaDrsBRMTPgL8BbqE4g74FODOte5miGvgeiirpIuDo9NIrgNuAn0p6maKxqvZFMAmYTRH0LuC/KD5w2wCfp6hNvAC8C/hU2tccirPwjZLWAo8AJ6V1q4H3A5dRXGJMBX7Zz2GdCDwqaV0q55kR8WpELAFOpmjQegFYQNF41tvvZTNFjWQ68BRFI+bVFNXlgY5/sD6TtruC4vd0A8WXYV/mAncAv6G4BNtAj2p+RPyS4gvuoYh45g1baENKjRBmWw1JlwOTIuLsOrfzc+D7EXF1Y0rWXD6zW9uTtL+kt6lwGEUD45w6t/kOij+n3tSIMraCbO8msq3KjhRV992B54B/oGhRHxJJ1wHvBc5Ll2Jbhbqq8ZJOpLj26gCujojLGlUwM2usIYc9tRT/hqIBahlwP3BWRDzWuOKZWaPUU40/DHgiIhYDSLqR4pbNPsPe2dkZe+21V58brH3xLFq0qHvZ+vXr6yiiWXsbN25c9/TUqVMBKG6T6N2SJUtYvXp1r0+oJ+x78Po/VyxjgD+d7LXXXtxzzz19rt+4cSMAJ5xwQvey+fPn11FEs/Z20EEHdU/PnTsXgNGj+/5/nKOOOqrPdcPeGi/pEyr+g+qB1atXD/fuzKwP9YR9Oa+/LXFKWvY6ETErImZGxMzOzs6eq81shNQT9vuBqZL2lbQtxV1jtzWmWGbWaEO+Zo+ITZI+Q3HrYQfw7Yh4tGElM7OGquummoj4McU/+ptZi/PtsmaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZGDDskr4taaWkR0rLxku6U9Ki9HPX4S2mmdWrypn9WoohfMsuAO6KiKnAXWnezFrYgGGPiF9QjMVddjpwXZquDYJnZi1sqNfsEyPi2TS9ApjYoPKY2TCpu4EuigHa+hwd0iPCmLWGoYb9OUmTAdLPlX090SPCmLWGoYb9NuDsNH02dQx8b2YjY8BBIiTdALwb6JS0DPgqcBlws6RzgGeAMxpZqClTpnRPr1q1qpGbNmsr5SzUa8CwR8RZfaw6tmGlMLNh5zvozDJR11hvjdbR0QHAueee271s7dq1zSqOWdPttNNO3dO1fAyVz+xmmWipM3vN2LFju6e3bNnSxJKYNVc5C/Xymd0sEw67WSZashpfbogYPXp0E0ti1lz1NsqV+cxulgmH3SwTLVWN32ab4rtnwoQJ3cvKf2c0y035MraWj6Hymd0sEy11Zq8pN0oU/y5vlic30JnZoDnsZplw2M0y4bCbZaLlG+g2b97cxJKYNdeINtBJ2lPSPEmPSXpU0nlpuUeFMWsjVarxm4AvRMQ04HDg05Km4VFhzNpKlRFhno2Ih9L0y0AXsAceFcasrQyqgU7SPsChwHwqjgrjQSLMWkPlBjpJOwC3AOdHxFpJ3esiIiT1eqtbRMwCZgHMmDGj0u1w5W3Xez+wWTsrZ6FelZIkaTRF0K+PiFvT4sqjwphZ81VpjRdwDdAVEd8orfKoMGZtpEo1/kjgQ8D/SlqQln2ZYRwVZvz48Y3alNlWY9OmTXW9vsqIMPcAfV04eFQYszbh1i+zTLTk7bLl3jka2Rpp1m7K/TnUW433md0sEw67WSYcdrNMOOxmmWjJBrrly5d3T9fbKGHWzkaN+l1E673/xGd2s0y05Jl9w4YNvU6b5WbMmDEN25bP7GaZcNjNMuGwm2XCYTfLhMNulomWao3fsmULAIsXL+5e9sILLzSrOGZNV/7b+qRJk4Chd9XmM7tZJlrqzF5TPpuvXOmu7cwaoUofdGMk3Sfpf9KIMBen5ftKmi/pCUk3Sdp2+ItrZkNVpRr/GnBMRBwCTAdOlHQ4cDnwzYh4K/AicM6wldLM6lZlRJiIiHVpdnR6BHAMMDst94gwZi2uar/xHaln2ZXAncCTwJqIqP1L2jKKIaF6e61HhDFrAZUa6CJiMzBd0i7AHGD/qjsYzIgwtf627r777u5lXV1dVXdlttU54IADuqdPOeWUurY1qD+9RcQaYB5wBLCLpNqXxRRgeV+vM7Pmq9Iav1s6oyNpe+B4ipFc5wHvS0/ziDBmLa5KNX4ycJ2kDoovh5sj4nZJjwE3SroUeJhiiKi61O6gW7BgQfey+fPn17tZs7b12muvdU/X8tHR0TGkbVUZEebXFMM091y+GDhsSHs1sxHn22XNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMlE57Kk76Ycl3Z7mPSKMWRsZzJn9PIqOJms8IoxZG6k6SMQU4BTg6jQvPCKMWVupemb/FvBFYEuan4BHhDFrK1X6jT8VWBkRDw5lBxExKyJmRsTMzs7OoWzCzBqgSr/xRwKnSToZGAPsBFxBGhEmnd09IoxZi6syiuuFETElIvYBzgR+HhEfwCPCmLWVev7O/iXg85KeoLiGr3tEGDMbPpVGca2JiLuBu9O0R4QxayO+g84sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sE5V6qpH0NPAysBnYFBEzJY0HbgL2AZ4GzoiIF4enmGZWr8Gc2Y+OiOkRMTPNXwDcFRFTgbvSvJm1qHqq8adTjAQDHhHGrOVVDXsAP5X0oKRPpGUTI+LZNL0CmNjbCz0ijFlrqNq77FERsVzSm4A7JS0sr4yIkBS9vTAiZgGzAGbMmNHrc8xs+FU6s0fE8vRzJTCHogvp5yRNBkg/Vw5XIc2sflXGehsnacfaNPBHwCPAbRQjwYBHhDFreVWq8ROBOcUozYwCvh8Rd0i6H7hZ0jnAM8AZw1dMM6vXgGFPI78c0svy54Fjh6NQZtZ4voPOLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBOVwi5pF0mzJS2U1CXpCEnjJd0paVH6uetwF9bMhq7qmf0K4I6I2J+ii6ouPCKMWVup0rvszsAfAtcARMT/RcQaPCKMWVupcmbfF1gFfEfSw5KuTl1Ke0QYszZSJeyjgBnAVRFxKLCeHlX2iAiKIaLeICJmRcTMiJjZ2dlZb3nNbIiqhH0ZsCwi5qf52RTh94gwZm1kwLBHxApgqaT90qJjgcfwiDBmbaXqwI7nAtdL2hZYDHyU4ovCI8KYtYlKYY+IBcDMXlZ5RBizNuE76Mwy4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0yUaXf+P0kLSg91ko63yPCmLWXKh1OPh4R0yNiOvB24BVgDh4RxqytDLYafyzwZEQ8g0eEMWsrgw37mcANadojwpi1kcphT91Inwb8R891HhHGrPUN5sx+EvBQRDyX5j0ijFkbGUzYz+J3VXjwiDBmbaVS2NOorccDt5YWXwYcL2kRcFyaN7MWVXVEmPXAhB7Lnscjwpi1Dd9BZ5YJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpaJqt1SfU7So5IekXSDpDGS9pU0X9ITkm5Kvc+aWYuqMvzTHsBngZkRcRDQQdF//OXANyPircCLwDnDWVAzq0+lPujS87aXtBEYCzwLHAP8WVp/HfA14Kr+NhIRbNy4sc/1tXWbN2+uWCxrZ6NHj+6enjRpUvd0R0dHQ7ZfG5Rk3bp1DdleM5Sz8OqrrwL0m6EtW7b0ua7KWG/Lgb8HllCE/CXgQWBNRGxKT1sG7NHb68sjwjz//PMD7c7MhsmAZ/Y0OuvpwL7AGooRYU6suoOImAXMAjj44IOjv8DXvrH6+3ayrceECb/rsPiiiy7qdXk9rrzySgDmzZvXkO01w4YNG7qnu7q6gP5rPuXn91Slge444KmIWBURGyn6jj8S2EVS7ctiCrC8wrbMrEmqhH0JcLiksZJE0Vf8Y8A84H3pOR4RxqzFDViNj4j5kmYDDwGbgIcpquU/Am6UdGladk2VHfZXRXf1PS/l9/uVV17pnh4zZkxDtt9fQ1a7KMZMLWzatKmfZ77x+T1VHRHmq8BXeyxeDBxW5fVm1nxV//Rm1nDlxtpLLrmke3rUqMZ8LNesWdOQ7WwtfLusWSYcdrNMqL8L+obvTFoFrAdWj9hOh18nPp5WtTUdC1Q7nr0jYrfeVoxo2AEkPRARM0d0p8PIx9O6tqZjgfqPx9V4s0w47GaZaEbYZzVhn8PJx9O6tqZjgTqPZ8Sv2c2sOVyNN8uEw26WiRENu6QTJT2e+q27YCT3XS9Je0qaJ+mx1B/feWn5eEl3SlqUfu7a7LIOhqQOSQ9Luj3Nt23fgpJ2kTRb0kJJXZKOaOf3p9F9P45Y2CV1AP8EnARMA86SNG2k9t8Am4AvRMQ04HDg06n8FwB3RcRU4K40307OA7pK8+3ct+AVwB0RsT9wCMVxteX7Myx9P0bEiDyAI4C5pfkLgQtHav/DcDw/AI4HHgcmp2WTgcebXbZBHMMUigAcA9wOiOIOrVG9vWet/AB2Bp4iNTqXlrfl+0PRzdtSYDzFP6zdDpxQz/szktX4WuFr+uy3rtVJ2gc4FJgPTIyIZ9OqFcDEZpVrCL4FfBGo/WP5BCr2LdiC9gVWAd9JlyVXSxpHm74/UWffj71xA90gSdoBuAU4PyLWltdF8XXbFn/LlHQqsDIiHmx2WRpkFDADuCoiDqX4H4zXVdnb7P0p9/24OzCOQfT92JuRDPtyYM/SfNv1WydpNEXQr4+IW9Pi5yRNTusnAyubVb5BOhI4TdLTwI0UVfkraN++BZcByyJifpqfTRH+dn1/Gt7340iG/X5gampN3JaiseG2Edx/XVL/e9cAXRHxjdKq2yj64IM26osvIi6MiCkRsQ/Fe/HziPgAbdq3YESsAJZK2i8tqvWV2JbvD8PR9+MINzqcDPwGeBL4SrMbQQZZ9qMoqoC/Bhakx8kU17l3AYuAnwHjm13WIRzbu4Hb0/SbgfuAJyi6Dd+u2eUbxHFMBx5I79F/Aru28/sDXAwsBB4BvgdsV8/749tlzTLhBjqzTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBP/DznaKYI1TMO8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "screen = em.get_processed_screen()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(screen.cpu().reshape(-1,84).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Processed screen example')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(screen.cpu().reshape(-1,84).numpy(),\n",
    "           interpolation='none', cmap = 'gray')\n",
    "plt.title('Processed screen gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7QYIIWd7zMQ"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tnBY8M28HTg"
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EgLSQ0E18KjZ"
   },
   "outputs": [],
   "source": [
    "folder_figs = \"figures\"\n",
    "os.makedirs(folder_figs, exist_ok=True)\n",
    "\n",
    "def plot_durations(values, moving_avg_period, episode):\n",
    "    plt.figure(1, figsize=(10,5))\n",
    "    plt.clf()  # Clear the current figure.\n",
    "    plt.xlabel('Episode', fontsize=14)\n",
    "    plt.ylabel('Duration', fontsize=14)\n",
    "    plt.plot(values)\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "    filename = os.path.join(folder_figs, \"durations_\" + version + \".png\")\n",
    "    plt.savefig(filename)\n",
    "    print(\"\", moving_avg_period, \"episode duration moving avg:\", moving_avg[-1])\n",
    "\n",
    "def plot_rewards(values, moving_avg_period, episode):\n",
    "    plt.figure(2, figsize=(10,5))\n",
    "    plt.clf()\n",
    "    plt.xlabel('Episode', fontsize=14)\n",
    "    plt.ylabel('Reward', fontsize=14)\n",
    "    plt.plot(values)\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "    filename = os.path.join(folder_figs, \"rewards_\" + version + \".png\")\n",
    "    plt.savefig(filename)\n",
    "    print(\"\", moving_avg_period, \"episode reward moving avg:\", moving_avg[-1])\n",
    "\n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "    return moving_avg.numpy()\n",
    "\n",
    "def plot_loss(values, episode):\n",
    "    plt.figure(3, figsize=(10,5))\n",
    "    plt.xlabel('Update', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.plot(values)\n",
    "    filename = os.path.join(folder_figs, \"loss_\" + version + \".png\")\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InDoPOi_C_rJ"
   },
   "source": [
    "### Tensor Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8UFM0mbcCmUc"
   },
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    \"\"\" Transposes the batch.\n",
    "    This converts batch-array of Experiences to Experience of batches (batch-arrays)\n",
    "    \"\"\"\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state).float().to(device)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = batch.next_state   # I don't want to concatenate all the next_states\n",
    "    t4 = torch.cat(batch.reward)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iJ-4Ru6EKBi"
   },
   "source": [
    "**Exapmple of `Experience(*zip(*experiences))` used above.**\n",
    "\n",
    "See https://stackoverflow.com/a/19343/3343043 for detailed explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2icDDJrADG6o",
    "outputId": "aabace72-f9b6-4f86-ed75-4dfa9e269a9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Experience(state=1, action=1, next_state=1, reward=1),\n",
       " Experience(state=2, action=2, next_state=2, reward=2),\n",
       " Experience(state=3, action=3, next_state=3, reward=3)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1 = Experience(1,1,1,1)\n",
    "e2 = Experience(2,2,2,2)\n",
    "e3 = Experience(3,3,3,3)\n",
    "\n",
    "experiences = [e1,e2,e3]\n",
    "experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bzv0adjoDZlB",
    "outputId": "00a0387a-65ec-4029-87d5-c80398e3adb4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=(1, 2, 3), action=(1, 2, 3), next_state=(1, 2, 3), reward=(1, 2, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = Experience(*zip(*experiences))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soHJM2fyDN85"
   },
   "source": [
    "## Q-Value Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zlZ0h6fnDO4K"
   },
   "outputs": [],
   "source": [
    "class QValues():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        \"\"\" Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken.\n",
    "            These are the actions which would've been taken for each batch state according to policy_net\n",
    "        \"\"\"\n",
    "        return policy_net(states).gather(dim=1, index=actions)\n",
    "\n",
    "    @staticmethod        \n",
    "    def get_next(policy_net, target_net, next_states):\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, next_states)),\n",
    "                                        device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in next_states if s is not None]).float().to(device)\n",
    "    \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        batch_size = len(next_states) # Since I didn't concatenate the states\n",
    "        values = torch.zeros(batch_size, device=QValues.device)\n",
    "        action = policy_net(non_final_next_states).detach().argmax(dim=1).view(-1,1)\n",
    "        values[non_final_mask] = target_net(non_final_next_states).detach().gather(1, action).view(-1)\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCiU9MC9-r4D"
   },
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AzRwt6wZ-tCv"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size          = 32\n",
    "gamma               = 0.99\n",
    "eps_start           = 1           #\n",
    "eps_end             = 0.1         # parameters for e-greedy strategy for action selection\n",
    "eps_decay           = 0.0000001   #\n",
    "optimize_model_step = 4           # Number of frames after which we train the model \n",
    "target_net_update   = 50          # Number of episodes after which we update the target model\n",
    "memory_size         = 200_000\n",
    "lr                  = 0.00001\n",
    "num_episodes        = 15_000\n",
    "timestep_max        = 18_000      # Number of timesteps after which we end an episode\n",
    "\n",
    "save_fig_step       = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhZR9lfP_w8d",
    "outputId": "f9b31a4b-ccee-4462-9b07-06510cd8ab16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Essential Objects\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "em           = EnvManager(env, device)\n",
    "strategy     = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent        = Agent(strategy, em.n_actions, device)\n",
    "memory       = ReplayMemory(memory_size)\n",
    "state_holder = StateHolder()\n",
    "\n",
    "policy_net   = DQN(em.get_screen_height(), em.get_screen_width(), em.n_actions).to(device)\n",
    "target_net   = DQN(em.get_screen_height(), em.get_screen_width(), em.n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()  # since we only use this net for inference\n",
    "\n",
    "optimizer = optim.Adam(params = policy_net.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCGPJ028Al1S"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMDrsJNEKuWc",
    "outputId": "52644319-6103-4ee0-e4cd-d679d2f57f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting date and time:  28/07/2021 15:42:12\n"
     ]
    }
   ],
   "source": [
    "# datetime object containing current date and time\n",
    "start = datetime.now()\n",
    "# format: dd/mm/YY H:M:S\n",
    "print(\"Starting date and time: \", start.strftime(\"%d/%m/%Y %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3fynOGbYACLc",
    "outputId": "1635aae3-069f-4103-fdb1-de0753377905"
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'filename_checkp' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-fb613cacc9af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtarget_net_update\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mexchange_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtot_steps_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0msave_vectors4plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_durations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-826de8c04fdc>\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(net, optimizer, episode, tot_steps_done)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m\"tot_steps_done\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtot_steps_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     }\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mfilename_checkp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_checkp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoint_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'filename_checkp' referenced before assignment"
     ]
    }
   ],
   "source": [
    "episode_durations = []\n",
    "episode_rewards = []\n",
    "losses = []\n",
    "tot_steps_done = 0\n",
    "\n",
    "policy_net.train()\n",
    "\n",
    "for episode in range(1, num_episodes + 1): # I prefer starting from 1\n",
    "    em.reset()\n",
    "    state_holder.push(em.get_state())\n",
    "    episode_reward = 0\n",
    "    lives = em.max_lives\n",
    "\n",
    "    for timestep in count():\n",
    "        # Transition handling code\n",
    "        state  = state_holder.get()\n",
    "        action = agent.select_action(tot_steps_done,state, policy_net)\n",
    "        reward, info = em.take_action(action)\n",
    "        episode_reward += reward\n",
    "        life = info['ale.lives'] # or em.env.ale.lives()\n",
    "        \n",
    "        state_holder.push(em.get_state())\n",
    "        next_state = state_holder.get()\n",
    "        \n",
    "        # Trick to significantly improve the convergence of training: if the episode did not end but the\n",
    "        # agent lost a life, then such a transition should be put in ReplayMemory as the final one.\n",
    "        # In this case, it is necessary to continue the episode until done == True\n",
    "        # At the same time, you will teach the agent that losing lives is bad.\n",
    "        \n",
    "        if not em.done:\n",
    "            if life < lives:\n",
    "                next_state, lives = (None, life)\n",
    "        else:\n",
    "            next_state = None\n",
    "            reward = torch.zeros_like(reward) # It has to be a Tensor!\n",
    "        memory.push(state.to('cpu'), action, next_state, reward)\n",
    "        state = next_state\n",
    "        \n",
    "        tot_steps_done += 1\n",
    "        \n",
    "        # Optimization step\n",
    "        if memory.can_provide_sample(batch_size) and tot_steps_done % optimize_model_step == 0:\n",
    "            experiences = memory.sample(batch_size)\n",
    "            \n",
    "            states, actions, next_states, rewards = extract_tensors(experiences)\n",
    "\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(policy_net, target_net, next_states)\n",
    "            # Compute the expected Q values using the Bellman's equation\n",
    "            target_q_values = rewards + (gamma * next_q_values)\n",
    "            \n",
    "            # Compute Huber loss\n",
    "            loss = F.smooth_l1_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            # loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Optimizing the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in policy_net.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Trying to save memory\n",
    "            del experiences\n",
    "            del states\n",
    "            del actions\n",
    "            del next_states\n",
    "            del rewards\n",
    "            del current_q_values\n",
    "            del next_q_values\n",
    "            del target_q_values\n",
    "            del loss\n",
    "        \n",
    "        if em.done:\n",
    "            episode_durations.append(timestep)\n",
    "            episode_rewards.append(episode_reward)\n",
    "            print(\"Episode\", episode)\n",
    "            print(\"Total steps done\", tot_steps_done)\n",
    "            if is_ipython: display.clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        if timestep > timestep_max:\n",
    "            break\n",
    "    \n",
    "    if episode % save_fig_step == 0:\n",
    "        plot_durations(episode_durations, 100, episode)\n",
    "        plot_rewards(episode_rewards, 100, episode)\n",
    "        plot_loss(losses, episode)\n",
    "\n",
    "    if episode % target_net_update == 0:\n",
    "        exchange_weights(target_net, policy_net)\n",
    "        save_checkpoint(policy_net, optimizer, episode, tot_steps_done)\n",
    "        save_vectors4plots(episode_durations, episode_rewards, losses)\n",
    "\n",
    "save_weights(policy_net, \"CNN_\" + version)\n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKjudcLDKuWe"
   },
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "# format: dd/mm/YY H:M:S\n",
    "print(f\"Finishing date and time: \", end.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print(f\"Total time training: {end-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A48a6HF2KuWf"
   },
   "source": [
    "Let's play an episode to see if it learned to play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBg83cX_KuWf"
   },
   "outputs": [],
   "source": [
    "#policy_net = DQN(em.get_screen_height(), em.get_screen_width(), em.n_actions).to(device)\n",
    "#load_weights(policy_net, \"CNN_\" + version + \".pt\")\n",
    "policy_net.eval()\n",
    "\n",
    "for episode in range(1):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    \n",
    "    for timestep in count():\n",
    "        em.render()\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        state = em.get_state()\n",
    "        if em.done:\n",
    "            break\n",
    "        \n",
    "em.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Breakout_CNN-Copy2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (RL virtualenv)",
   "language": "python",
   "name": "reinforcementl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
