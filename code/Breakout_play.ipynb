{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdspU5QVN7BQ"
   },
   "source": [
    "# Play Breakout using the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpSZJAb1uNUd"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hgE0d2CsRVEp"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os                                      # to create folders\n",
    "import gym                                     # contains the game environment\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from datetime import datetime                  # to print a timestamp\n",
    "import pickle                                  # to save on file\n",
    "import torch                                   # ANNs\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nPvkGXSkKuV7"
   },
   "outputs": [],
   "source": [
    "# Crea l'ambiente con il gioco\n",
    "\n",
    "#env = gym.make('Breakout-v0').unwrapped\n",
    "env = gym.make('BreakoutDeterministic-v4').unwrapped\n",
    "#env = gym.make('BreakoutNoFrameskip-v4').unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaLGz1W2uV0h"
   },
   "source": [
    "## Set Up Device\n",
    "\n",
    "We import IPython's display module to aid us in plotting images to the screen later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "erns3ihuNtca"
   },
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoH0D_-6uvwl"
   },
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nUUdYuRARtGl"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Initialize a deep Q-learning network\n",
    "    \n",
    "    Hints:\n",
    "    -----\n",
    "        Original paper for DQN\n",
    "    https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, img_height, img_width, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d\n",
    "        def conv2d_size_out(size, kernel_size, stride=1, padding=0):\n",
    "            return int(size + 2 * padding - kernel_size) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(\n",
    "                    conv2d_size_out(\n",
    "                        conv2d_size_out(img_height, kernel_size=8, stride=4\n",
    "                    ), kernel_size=4, stride=2\n",
    "                ), kernel_size=3, stride=1)\n",
    "        convh = conv2d_size_out(\n",
    "                    conv2d_size_out(\n",
    "                        conv2d_size_out(img_width, kernel_size=8, stride=4\n",
    "                    ), kernel_size=4, stride=2\n",
    "                ), kernel_size=3, stride=1)\n",
    "        \n",
    "        linear_input_size = convw * convh * 64  # = 7 * 7 * 64 = 3136\n",
    "        \n",
    "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Calculates probability of each action.\n",
    "        Called with either one element to determine next action, or a batch during optimization.\n",
    "        NOTE: a single discrete state is collection of 4 frames\n",
    "        :param x: processed state of shape b x 4 x 84 x 84\n",
    "        :returns tensor of shape [batch_size, n_actions] (estimated action values)\n",
    "        \"\"\"\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))  # b x 32 x 20 x 20\n",
    "        x = F.relu(self.conv2(x))  # b x 64 x 9 x 9\n",
    "        x = F.relu(self.conv3(x))  # b x 64 x 7 x 7\n",
    "        x = x.view(x.size(0), -1)  # b x (7 * 7 * 64) x 1\n",
    "        x = F.relu(self.fc1(x))    # b x 512\n",
    "        x = self.fc2(x)            # b x  4\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StateHolder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateHolder:\n",
    "    \"\"\" Class which stores the state of the game.\n",
    "    We will use 4 consecutive frames of the game stacked together.\n",
    "    This is necessary for the agent to understand the speed and acceleration of game objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, number_screens = 4):\n",
    "        self.first_action = True\n",
    "        self.state = torch.ByteTensor(1, 84, 84).to(device)\n",
    "        self.number_screens = number_screens\n",
    "        \n",
    "    def push(self, screen):\n",
    "        new_screen = screen.squeeze(0)\n",
    "        if self.first_action:\n",
    "            self.state[0] = new_screen\n",
    "            for number in range(self.number_screens-1):\n",
    "                self.state = torch.cat((self.state, new_screen), 0)\n",
    "            self.first_action = False\n",
    "        else:\n",
    "            self.state = torch.cat((self.state, new_screen), 0)[1:]\n",
    "    \n",
    "    def get(self):\n",
    "        return self.state.unsqueeze(0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.first_action = True\n",
    "        self.state = torch.ByteTensor(1, 84, 84).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEcJ15vIx8Hz"
   },
   "source": [
    "## Epsilon Greedy Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "A1vPemxix8gc"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, agent_current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * agent_current_step * self.decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9Ko3XqYyqHd"
   },
   "source": [
    "## Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sqWEsCB1yq0o"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.strategy     = strategy\n",
    "        self.num_actions  = num_actions # number of actions that can be taken from a given state\n",
    "        self.device       = device\n",
    "\n",
    "    def select_action(self, current_step, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(current_step)\n",
    "\n",
    "        if rate > random.random() and state is not None:\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([[action]], device=self.device, dtype=torch.long) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():  # since it's not training\n",
    "                return policy_net(state.float()).argmax(dim=1).to(self.device).view(1, 1) # exploit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKWuHrbQ2Fzl"
   },
   "source": [
    "## Environment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OJ3kpCox2Ghq"
   },
   "outputs": [],
   "source": [
    "STATE_W = 84\n",
    "STATE_H = 84\n",
    "\n",
    "class EnvManager():\n",
    "\n",
    "    def __init__(self, env, device):\n",
    "        self.device = device\n",
    "        self.env = env\n",
    "        self.env.reset() # to have an initial observation of the env\n",
    "        self.max_lives = self.env.ale.lives()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Resets the env to the initial state\n",
    "        \"\"\"\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Closes the env\n",
    "        \"\"\"\n",
    "        self.env.close()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "    def take_action(self, action):        \n",
    "        _, reward, self.done, info = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device), info\n",
    "\n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\" Returns the current state of the env in the form of a procesed image of the screen\n",
    "        \"\"\"\n",
    "        s = self.get_processed_screen()\n",
    "        self.current_screen = s\n",
    "        return s\n",
    "\n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "\n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "\n",
    "    def get_processed_screen(self):\n",
    "        screen = self.render(mode='rgb_array')\n",
    "        screen = np.dot(screen[...,:3], [0.299, 0.587, 0.114])\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "\n",
    "    def crop_screen(self, screen):\n",
    "        # Strip off top and bottom\n",
    "        return screen[32:195,:]\n",
    "\n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to uint, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.uint8).reshape(screen.shape[0],screen.shape[1],1)\n",
    "\n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((STATE_W, STATE_H)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        return resize(screen).mul(255).type(torch.ByteTensor).to(device).detach().unsqueeze(0) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCiU9MC9-r4D"
   },
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "eps_start           = 1           #\n",
    "eps_end             = 0.1         # parameters for e-greedy strategy for action selection\n",
    "eps_decay           = 0.0000001   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhZR9lfP_w8d",
    "outputId": "f9b31a4b-ccee-4462-9b07-06510cd8ab16"
   },
   "outputs": [],
   "source": [
    "# Essential Objects\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "em           = EnvManager(env, device)\n",
    "strategy     = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent        = Agent(strategy, em.n_actions, device)\n",
    "state_holder = StateHolder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aSTUo2AUKuWf"
   },
   "outputs": [],
   "source": [
    "# restore checkpoint\n",
    "version = \"02\"\n",
    "checkp_number = 41550\n",
    "\n",
    "folder_save = \"models\"\n",
    "folder_checkp = os.path.join(folder_save, \"checkpoints_\" + version)\n",
    "\n",
    "filename_checkpoint = os.path.join(folder_checkp, \"checkpoint_\" + str(checkp_number) + \".pt\")\n",
    "checkpoint = torch.load(filename_checkpoint)\n",
    "\n",
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width(), em.n_actions).to(device)\n",
    "policy_net.load_state_dict(checkpoint[\"parameters\"])\n",
    "\n",
    "episode_train   = checkpoint[\"episode\"]\n",
    "tot_steps_train = checkpoint[\"tot_steps_done\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for 41550 episodes. Total steps done: 11438008\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained for\", episode_train, \"episodes. Total steps done:\", tot_steps_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCGPJ028Al1S"
   },
   "source": [
    "### Play an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A48a6HF2KuWf"
   },
   "source": [
    "Let's play an episode to see if it learned to play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MBg83cX_KuWf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 11.0\n",
      "Total steps done 409\n"
     ]
    }
   ],
   "source": [
    "policy_net.eval()\n",
    "\n",
    "tot_steps_done = tot_steps_train\n",
    "\n",
    "for episode in range(1):\n",
    "    em.reset()\n",
    "    state_holder.push(em.get_state())\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for timestep in count():\n",
    "        em.render()\n",
    "        \n",
    "        state  = state_holder.get()\n",
    "        action = agent.select_action(tot_steps_done, state, policy_net)\n",
    "        reward, info = em.take_action(action)\n",
    "        episode_reward += reward.item()\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        state_holder.push(em.get_state())\n",
    "        state = state_holder.get()\n",
    "        \n",
    "        tot_steps_done += 1\n",
    "        \n",
    "        if em.done:\n",
    "            print(\"Reward\", episode_reward)\n",
    "            print(\"Total steps done\", timestep)\n",
    "            break\n",
    "        \n",
    "em.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Breakout_CNN-Copy2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (RL virtualenv)",
   "language": "python",
   "name": "reinforcementl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
