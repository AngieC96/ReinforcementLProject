{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdspU5QVN7BQ"
   },
   "source": [
    "# Play Breakout using the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpSZJAb1uNUd"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hgE0d2CsRVEp"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os                                      # to create folders\n",
    "import gym                                     # contains the game environment\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from datetime import datetime                  # to print a timestamp\n",
    "import pickle                                  # to save on file\n",
    "import torch                                   # ANNs\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nPvkGXSkKuV7"
   },
   "outputs": [],
   "source": [
    "# Crea l'ambiente con il gioco\n",
    "\n",
    "#env = gym.make('Breakout-v0').unwrapped\n",
    "env = gym.make('BreakoutDeterministic-v4').unwrapped\n",
    "#env = gym.make('BreakoutNoFrameskip-v4').unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaLGz1W2uV0h"
   },
   "source": [
    "## Set Up Device\n",
    "\n",
    "We import IPython's display module to aid us in plotting images to the screen later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "erns3ihuNtca"
   },
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoH0D_-6uvwl"
   },
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nUUdYuRARtGl"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Initialize a deep Q-learning network\n",
    "    \n",
    "    Hints:\n",
    "    -----\n",
    "        Original paper for DQN\n",
    "    https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, img_height, img_width, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d\n",
    "        def conv2d_size_out(size, kernel_size, stride=1, padding=0):\n",
    "            return int(size + 2 * padding - kernel_size) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(\n",
    "                    conv2d_size_out(\n",
    "                        conv2d_size_out(img_height, kernel_size=8, stride=4\n",
    "                    ), kernel_size=4, stride=2\n",
    "                ), kernel_size=3, stride=1)\n",
    "        convh = conv2d_size_out(\n",
    "                    conv2d_size_out(\n",
    "                        conv2d_size_out(img_width, kernel_size=8, stride=4\n",
    "                    ), kernel_size=4, stride=2\n",
    "                ), kernel_size=3, stride=1)\n",
    "        \n",
    "        linear_input_size = convw * convh * 64  # = 7 * 7 * 64 = 3136\n",
    "        \n",
    "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Calculates probability of each action.\n",
    "        Called with either one element to determine next action, or a batch during optimization.\n",
    "        NOTE: a single discrete state is collection of 4 frames\n",
    "        :param x: processed state of shape b x 4 x 84 x 84\n",
    "        :returns tensor of shape [batch_size, n_actions] (estimated action values)\n",
    "        \"\"\"\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))  # b x 32 x 20 x 20\n",
    "        x = F.relu(self.conv2(x))  # b x 64 x 9 x 9\n",
    "        x = F.relu(self.conv3(x))  # b x 64 x 7 x 7\n",
    "        x = x.view(x.size(0), -1)  # b x (7 * 7 * 64) x 1\n",
    "        x = F.relu(self.fc1(x))    # b x 512\n",
    "        x = self.fc2(x)            # b x  4\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StateHolder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateHolder:\n",
    "    \"\"\" Class which stores the state of the game.\n",
    "    We will use 4 consecutive frames of the game stacked together.\n",
    "    This is necessary for the agent to understand the speed and acceleration of game objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, number_screens = 4):\n",
    "        self.first_action = True\n",
    "        self.state = torch.ByteTensor(1, 84, 84).to(device)\n",
    "        self.number_screens = number_screens\n",
    "        \n",
    "    def push(self, screen):\n",
    "        new_screen = screen.squeeze(0)\n",
    "        if self.first_action:\n",
    "            self.state[0] = new_screen\n",
    "            for number in range(self.number_screens-1):\n",
    "                self.state = torch.cat((self.state, new_screen), 0)\n",
    "            self.first_action = False\n",
    "        else:\n",
    "            self.state = torch.cat((self.state, new_screen), 0)[1:]\n",
    "    \n",
    "    def get(self):\n",
    "        return self.state.unsqueeze(0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.first_action = True\n",
    "        self.state = torch.ByteTensor(1, 84, 84).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEcJ15vIx8Hz"
   },
   "source": [
    "## Epsilon Greedy Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A1vPemxix8gc"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, agent_current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * agent_current_step * self.decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9Ko3XqYyqHd"
   },
   "source": [
    "## Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sqWEsCB1yq0o"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.strategy     = strategy\n",
    "        self.num_actions  = num_actions # number of actions that can be taken from a given state\n",
    "        self.device       = device\n",
    "\n",
    "    def select_action(self, current_step, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(current_step)\n",
    "\n",
    "        if rate > random.random() and state is not None:\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([[action]], device=self.device, dtype=torch.long) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():  # since it's not training\n",
    "                return policy_net(state.float()).argmax(dim=1).to(self.device).view(1, 1) # exploit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKWuHrbQ2Fzl"
   },
   "source": [
    "## Environment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OJ3kpCox2Ghq"
   },
   "outputs": [],
   "source": [
    "STATE_W = 84\n",
    "STATE_H = 84\n",
    "\n",
    "class EnvManager():\n",
    "\n",
    "    def __init__(self, env, device):\n",
    "        self.device = device\n",
    "        self.env = env\n",
    "        self.env.reset() # to have an initial observation of the env\n",
    "        self.max_lives = self.env.ale.lives()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Resets the env to the initial state\n",
    "        \"\"\"\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Closes the env\n",
    "        \"\"\"\n",
    "        self.env.close()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "    def take_action(self, action):        \n",
    "        _, reward, self.done, info = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device), info\n",
    "\n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\" Returns the current state of the env in the form of a procesed image of the screen\n",
    "        \"\"\"\n",
    "        s = self.get_processed_screen()\n",
    "        self.current_screen = s\n",
    "        return s\n",
    "\n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "\n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "\n",
    "    def get_processed_screen(self):\n",
    "        screen = self.render(mode='rgb_array')\n",
    "        screen = np.dot(screen[...,:3], [0.299, 0.587, 0.114])\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "\n",
    "    def crop_screen(self, screen):\n",
    "        # Strip off top and bottom\n",
    "        return screen[32:195,:]\n",
    "\n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to uint, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.uint8).reshape(screen.shape[0],screen.shape[1],1)\n",
    "\n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((STATE_W, STATE_H)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        return resize(screen).mul(255).type(torch.ByteTensor).to(device).detach().unsqueeze(0) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7QYIIWd7zMQ"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tnBY8M28HTg"
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EgLSQ0E18KjZ"
   },
   "outputs": [],
   "source": [
    "folder_figs = \"figures\"\n",
    "os.makedirs(folder_figs, exist_ok=True)\n",
    "\n",
    "def plot_durations(values, moving_avg_period, episode):\n",
    "    plt.figure(1, figsize=(10,5))\n",
    "    plt.clf()  # Clear the current figure.\n",
    "    plt.xlabel('Episode', fontsize=14)\n",
    "    plt.ylabel('Duration', fontsize=14)\n",
    "    plt.plot(values)\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "    filename = os.path.join(folder_figs, \"durations_\" + version + \".png\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    print(\"Episode\", episode + len(values), \"\\n\", \\\n",
    "        moving_avg_period, \"episode duration moving avg:\", moving_avg[-1])\n",
    "    #if is_ipython: display.clear_output(wait=True)\n",
    "\n",
    "def plot_rewards(values, moving_avg_period, episode):\n",
    "    plt.figure(2, figsize=(10,5))\n",
    "    plt.clf()\n",
    "    plt.xlabel('Episode', fontsize=14)\n",
    "    plt.ylabel('Reward', fontsize=14)\n",
    "    plt.plot(values)\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "    filename = os.path.join(folder_figs, \"rewards_\" + version + \".png\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    print(\"Episode\", episode + len(values), \"\\n\", \\\n",
    "        moving_avg_period, \"episode reward moving avg:\", moving_avg[-1])\n",
    "    #if is_ipython: display.clear_output(wait=True)\n",
    "\n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "    return moving_avg.numpy()\n",
    "\n",
    "def plot_loss(values, steps):\n",
    "    plt.figure(3, figsize=(10,5))\n",
    "    plt.xlabel('Update', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.plot(values)\n",
    "    filename = os.path.join(folder_figs, \"loss_\" + version + \".png\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    if is_ipython: display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCiU9MC9-r4D"
   },
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "eps_start           = 1           #\n",
    "eps_end             = 0.1         # parameters for e-greedy strategy for action selection\n",
    "eps_decay           = 0.0000001   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhZR9lfP_w8d",
    "outputId": "f9b31a4b-ccee-4462-9b07-06510cd8ab16"
   },
   "outputs": [],
   "source": [
    "# Essential Objects\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "em           = EnvManager(env, device)\n",
    "strategy     = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent        = Agent(strategy, em.n_actions, device)\n",
    "state_holder = StateHolder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aSTUo2AUKuWf"
   },
   "outputs": [],
   "source": [
    "# restore checkpoint\n",
    "version = \"03\"\n",
    "checkp_number = 750\n",
    "\n",
    "folder_save = \"models\"\n",
    "folder_checkp = \"checkpoints_\" + version\n",
    "filename_checkp = os.path.join(folder_save, folder_checkp)\n",
    "\n",
    "filename_checkpoint = os.path.join(filename_checkp, \"checkpoint_\" + str(checkp_number) + \".pt\")\n",
    "checkpoint = torch.load(filename_checkpoint)\n",
    "\n",
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width(), em.n_actions).to(device)\n",
    "policy_net.load_state_dict(checkpoint[\"parameters\"])\n",
    "\n",
    "episode_train   = checkpoint[\"episode\"]\n",
    "tot_steps_train = checkpoint[\"tot_steps_done\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for 15000 episodes. Total step done: 133543\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained for\", episode_train, \"episodes. Total steps done:\", tot_steps_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_durations = \"durations.pickle\"\n",
    "filename_rewards   = \"rewards.pickle\"\n",
    "filename_losses    = \"losses.pickle\"\n",
    "\n",
    "# restore arrays of durations, rewards and losses\n",
    "filename_durations = os.path.join(filename_checkp, filename_durations)\n",
    "infile_durations = open(filename_durations, 'rb')\n",
    "episode_durations = pickle.load(infile_durations)\n",
    "infile_durations.close()\n",
    "\n",
    "filename_rewards = os.path.join(filename_checkp, filename_rewards)\n",
    "infile_rewards = open(filename_rewards, 'rb')\n",
    "episode_rewards = pickle.load(infile_rewards)\n",
    "infile_rewards.close()\n",
    "\n",
    "filename_losses = os.path.join(filename_checkp, filename_losses)\n",
    "infile_losses = open(filename_losses, 'rb')\n",
    "losses = pickle.load(infile_losses)\n",
    "infile_losses.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 750 33371\n"
     ]
    }
   ],
   "source": [
    "print(len(episode_durations), len(episode_rewards), len(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCGPJ028Al1S"
   },
   "source": [
    "### Play an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A48a6HF2KuWf"
   },
   "source": [
    "Let's play an episode to see if it learned to play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "MBg83cX_KuWf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 1.0\n",
      "Total steps done 155\n"
     ]
    }
   ],
   "source": [
    "policy_net.eval()\n",
    "\n",
    "tot_steps_done = tot_steps_train\n",
    "\n",
    "for episode in range(1):\n",
    "    em.reset()\n",
    "    state_holder.push(em.get_state())\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for timestep in count():\n",
    "        em.render()\n",
    "        \n",
    "        state  = state_holder.get()\n",
    "        action = agent.select_action(tot_steps_done, state, policy_net)\n",
    "        reward, info = em.take_action(action)\n",
    "        episode_reward += reward.item()\n",
    "        \n",
    "        state_holder.push(em.get_state())\n",
    "        state = state_holder.get()\n",
    "        \n",
    "        tot_steps_done += 1\n",
    "        \n",
    "        if em.done:\n",
    "            print(\"Reward\", episode_reward)\n",
    "            print(\"Total steps done\", timestep)\n",
    "            break\n",
    "        \n",
    "em.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Breakout_CNN-Copy2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (RL virtualenv)",
   "language": "python",
   "name": "reinforcementl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
