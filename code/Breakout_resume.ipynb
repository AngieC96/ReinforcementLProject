{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdspU5QVN7BQ"
   },
   "source": [
    "# Resume Breakout training with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpSZJAb1uNUd"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hgE0d2CsRVEp"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os                                      # to create folders\n",
    "import gym                                     # contains the game environment\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from datetime import datetime                  # to print a timestamp\n",
    "import pickle                                  # to save on file\n",
    "import torch                                   # ANNs\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IJB_FAOiKuV5"
   },
   "outputs": [],
   "source": [
    "# Tracking version for saving weights\n",
    "version = \"01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nPvkGXSkKuV7"
   },
   "outputs": [],
   "source": [
    "# Crea l'ambiente con il gioco\n",
    "\n",
    "#env = gym.make('Breakout-v0').unwrapped\n",
    "env = gym.make('BreakoutDeterministic-v4').unwrapped\n",
    "#env = gym.make('BreakoutNoFrameskip-v4').unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaLGz1W2uV0h"
   },
   "source": [
    "## Set Up Device\n",
    "\n",
    "We import IPython's display module to aid us in plotting images to the screen later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "erns3ihuNtca"
   },
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoH0D_-6uvwl"
   },
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nUUdYuRARtGl"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Initialize a deep Q-learning network\n",
    "    \n",
    "    Hints:\n",
    "    -----\n",
    "        Original paper for DQN\n",
    "    https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, img_height, img_width, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d\n",
    "        def conv2d_size_out(size, kernel_size, stride=1, padding=0):\n",
    "            return int(size + 2 * padding - kernel_size) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(\n",
    "                    conv2d_size_out(\n",
    "                        conv2d_size_out(img_height, kernel_size=8, stride=4\n",
    "                    ), kernel_size=4, stride=2\n",
    "                ), kernel_size=3, stride=1)\n",
    "        convh = conv2d_size_out(\n",
    "                    conv2d_size_out(\n",
    "                        conv2d_size_out(img_width, kernel_size=8, stride=4\n",
    "                    ), kernel_size=4, stride=2\n",
    "                ), kernel_size=3, stride=1)\n",
    "        \n",
    "        linear_input_size = convw * convh * 64  # = 7 * 7 * 64 = 3136\n",
    "        \n",
    "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Calculates probability of each action.\n",
    "        Called with either one element to determine next action, or a batch during optimization.\n",
    "        NOTE: a single discrete state is collection of 4 frames\n",
    "        :param x: processed state of shape b x 4 x 84 x 84\n",
    "        :returns tensor of shape [batch_size, n_actions] (estimated action values)\n",
    "        \"\"\"\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))  # b x 32 x 20 x 20\n",
    "        x = F.relu(self.conv2(x))  # b x 64 x 9 x 9\n",
    "        x = F.relu(self.conv3(x))  # b x 64 x 7 x 7\n",
    "        x = x.view(x.size(0), -1)  # b x (7 * 7 * 64) x 1\n",
    "        x = F.relu(self.fc1(x))    # b x 512\n",
    "        x = self.fc2(x)            # b x  4\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Dndx7phTKuWC"
   },
   "outputs": [],
   "source": [
    "folder_save = \"models\"\n",
    "os.makedirs(folder_save, exist_ok=True)\n",
    "folder_checkp = os.path.join(folder_save, \"checkpoints_\" + version)\n",
    "os.makedirs(folder_checkp, exist_ok=True)\n",
    "\n",
    "filename_durations = os.path.join(folder_checkp, \"durations.pickle\")\n",
    "filename_rewards   = os.path.join(folder_checkp, \"rewards.pickle\")\n",
    "filename_losses    = os.path.join(folder_checkp, \"losses.pickle\")\n",
    "\n",
    "def exchange_weights(net1, net2):\n",
    "    net1.load_state_dict(net2.state_dict())\n",
    "\n",
    "def load_weights(net, filename):\n",
    "    filename = os.path.join(folder_save, filename)\n",
    "    net.load_state_dict(torch.load(filename))\n",
    "\n",
    "def save_weights(net, filename: str):\n",
    "    filename = os.path.join(folder_save, filename + \".pt\")\n",
    "    torch.save(net.state_dict(), filename)\n",
    "    \n",
    "def save_checkpoint(net, optimizer, episode, tot_steps_done):\n",
    "    checkpoint_dict = {\n",
    "        \"parameters\"    : net.state_dict(),\n",
    "        \"optimizer\"     : optimizer.state_dict(),\n",
    "        \"episode\"       : episode,\n",
    "        \"tot_steps_done\": tot_steps_done\n",
    "    }\n",
    "    filename_checkp = os.path.join(folder_checkp, \"checkpoint_\" + str(episode) +\".pt\")\n",
    "    torch.save(checkpoint_dict, filename_checkp)\n",
    "    \n",
    "def save_vectors4plots(episode_durations, episode_rewards, losses):\n",
    "    # save the vectors\n",
    "    outfile = open(filename_durations, 'wb')\n",
    "    pickle.dump(episode_durations, outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "    outfile = open(filename_rewards, 'wb')\n",
    "    pickle.dump(episode_rewards, outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "    outfile = open(filename_losses, 'wb')\n",
    "    pickle.dump(losses, outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFmilqPxv6EN"
   },
   "source": [
    "## Experience class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fwgsUELpRuH9"
   },
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TM3kAq15R9WY",
    "outputId": "6002ad1c-8d97-4da2-f682-47356c32ec00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=2, action=3, next_state=1, reward=4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Experience(2,3,1,4)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UshQ99EvKuWH"
   },
   "source": [
    "## StateHolder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sEwxXzh9KuWI"
   },
   "outputs": [],
   "source": [
    "class StateHolder:\n",
    "    \"\"\" Class which stores the state of the game.\n",
    "    We will use 4 consecutive frames of the game stacked together.\n",
    "    This is necessary for the agent to understand the speed and acceleration of game objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, number_screens = 4):\n",
    "        self.first_action = True\n",
    "        self.state = torch.ByteTensor(1, 84, 84).to(device)\n",
    "        self.number_screens = number_screens\n",
    "        \n",
    "    def push(self, screen):\n",
    "        new_screen = screen.squeeze(0)\n",
    "        if self.first_action:\n",
    "            self.state[0] = new_screen\n",
    "            for number in range(self.number_screens-1):\n",
    "                self.state = torch.cat((self.state, new_screen), 0)\n",
    "            self.first_action = False\n",
    "        else:\n",
    "            self.state = torch.cat((self.state, new_screen), 0)[1:]\n",
    "    \n",
    "    def get(self):\n",
    "        return self.state.unsqueeze(0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.first_action = True\n",
    "        self.state = torch.ByteTensor(1, 84, 84).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4bIgT98wbcJ"
   },
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UgbpipIRR-IC"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "  \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0       # Number of experiences added to the memory\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(Experience(*args))\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = Experience(*args)  # overwrite the first experiences first\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size    # can we sample from memory?\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEcJ15vIx8Hz"
   },
   "source": [
    "## Epsilon Greedy Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A1vPemxix8gc"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, agent_current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * agent_current_step * self.decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9Ko3XqYyqHd"
   },
   "source": [
    "## Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sqWEsCB1yq0o"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.strategy     = strategy\n",
    "        self.num_actions  = num_actions # number of actions that can be taken from a given state\n",
    "        self.device       = device\n",
    "\n",
    "    def select_action(self, current_step, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(current_step)\n",
    "\n",
    "        if rate > random.random() and state is not None:\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([[action]], device=self.device, dtype=torch.long) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():  # since it's not training\n",
    "                return policy_net(state.float()).argmax(dim=1).to(self.device).view(1, 1) # exploit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKWuHrbQ2Fzl"
   },
   "source": [
    "## Environment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OJ3kpCox2Ghq"
   },
   "outputs": [],
   "source": [
    "STATE_W = 84\n",
    "STATE_H = 84\n",
    "\n",
    "class EnvManager():\n",
    "\n",
    "    def __init__(self, env, device):\n",
    "        self.device = device\n",
    "        self.env = env\n",
    "        self.env.reset() # to have an initial observation of the env\n",
    "        self.max_lives = self.env.ale.lives()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Resets the env to the initial state\n",
    "        \"\"\"\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Closes the env\n",
    "        \"\"\"\n",
    "        self.env.close()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "    def take_action(self, action):        \n",
    "        _, reward, self.done, info = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device), info\n",
    "\n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\" Returns the current state of the env in the form of a procesed image of the screen\n",
    "        \"\"\"\n",
    "        s = self.get_processed_screen()\n",
    "        self.current_screen = s\n",
    "        return s\n",
    "\n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "\n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "\n",
    "    def get_processed_screen(self):\n",
    "        screen = self.render(mode='rgb_array')\n",
    "        screen = np.dot(screen[...,:3], [0.299, 0.587, 0.114])\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "\n",
    "    def crop_screen(self, screen):\n",
    "        # Strip off top and bottom\n",
    "        return screen[32:195,:]\n",
    "\n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to uint, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.uint8).reshape(screen.shape[0],screen.shape[1],1)\n",
    "\n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((STATE_W, STATE_H)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        return resize(screen).mul(255).type(torch.ByteTensor).to(device).detach().unsqueeze(0) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7QYIIWd7zMQ"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tnBY8M28HTg"
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EgLSQ0E18KjZ"
   },
   "outputs": [],
   "source": [
    "folder_figs = \"figures\"\n",
    "os.makedirs(folder_figs, exist_ok=True)\n",
    "\n",
    "def plot_durations(values, moving_avg_period):\n",
    "    plt.figure(1, figsize=(10,5))\n",
    "    plt.clf()  # Clear the current figure.\n",
    "    plt.xlabel('Episode', fontsize=14)\n",
    "    plt.ylabel('Duration', fontsize=14)\n",
    "    plt.plot(values)\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "    filename = os.path.join(folder_figs, \"durations_\" + version + \".png\")\n",
    "    plt.savefig(filename)\n",
    "    print(\"\", moving_avg_period, \"episode duration moving avg:\", moving_avg[-1])\n",
    "\n",
    "def plot_rewards(values, moving_avg_period):\n",
    "    plt.figure(2, figsize=(10,5))\n",
    "    plt.clf()\n",
    "    plt.xlabel('Episode', fontsize=14)\n",
    "    plt.ylabel('Reward', fontsize=14)\n",
    "    plt.plot(values)\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "    filename = os.path.join(folder_figs, \"rewards_\" + version + \".png\")\n",
    "    plt.savefig(filename)\n",
    "    print(\"\", moving_avg_period, \"episode reward moving avg:\", moving_avg[-1])\n",
    "\n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "    return moving_avg.numpy()\n",
    "\n",
    "def plot_loss(values):\n",
    "    plt.figure(3, figsize=(10,5))\n",
    "    plt.xlabel('Update', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.plot(values)\n",
    "    filename = os.path.join(folder_figs, \"loss_\" + version + \".png\")\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InDoPOi_C_rJ"
   },
   "source": [
    "### Tensor Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8UFM0mbcCmUc"
   },
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    \"\"\" Transposes the batch.\n",
    "    This converts batch-array of Experiences to Experience of batches (batch-arrays)\n",
    "    \"\"\"\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state).float().to(device)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = batch.next_state   # I don't want to concatenate all the next_states\n",
    "    t4 = torch.cat(batch.reward)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soHJM2fyDN85"
   },
   "source": [
    "## Q-Value Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zlZ0h6fnDO4K"
   },
   "outputs": [],
   "source": [
    "class QValues():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        \"\"\" Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken.\n",
    "            These are the actions which would've been taken for each batch state according to policy_net\n",
    "        \"\"\"\n",
    "        return policy_net(states).gather(dim=1, index=actions)\n",
    "\n",
    "    @staticmethod        \n",
    "    def get_next(policy_net, target_net, next_states):\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, next_states)),\n",
    "                                        device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in next_states if s is not None]).float().to(device)\n",
    "    \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        batch_size = len(next_states) # len instead of .shape since I didn't concatenate the states\n",
    "        values = torch.zeros(batch_size, device=QValues.device)\n",
    "        action = policy_net(non_final_next_states).detach().argmax(dim=1).view(-1,1)\n",
    "        values[non_final_mask] = target_net(non_final_next_states).detach().gather(1, action).view(-1)\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCiU9MC9-r4D"
   },
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AzRwt6wZ-tCv"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size          = 32\n",
    "gamma               = 0.99\n",
    "eps_start           = 1           #\n",
    "eps_end             = 0.1         # parameters for e-greedy strategy for action selection\n",
    "eps_decay           = 0.0000001   #\n",
    "optimize_model_step = 4           # Number of frames after which we train the model \n",
    "target_net_update   = 50          # Number of episodes after which we update the target model\n",
    "memory_size         = 200_000\n",
    "lr                  = 0.00001\n",
    "num_episodes        = 15_000\n",
    "timestep_max        = 18_000      # Number of timesteps after which we end an episode\n",
    "\n",
    "save_fig_step       = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhZR9lfP_w8d",
    "outputId": "f9b31a4b-ccee-4462-9b07-06510cd8ab16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Essential Objects\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "em           = EnvManager(env, device)\n",
    "strategy     = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent        = Agent(strategy, em.n_actions, device)\n",
    "memory       = ReplayMemory(memory_size)\n",
    "state_holder = StateHolder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aSTUo2AUKuWf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restore checkpoint\n",
    "checkp_number = 250\n",
    "\n",
    "filename_checkpoint = os.path.join(folder_checkp, \"checkpoint_\" + str(checkp_number) + \".pt\")\n",
    "checkpoint = torch.load(filename_checkpoint)\n",
    "\n",
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width(), em.n_actions).to(device)\n",
    "policy_net.load_state_dict(checkpoint[\"parameters\"])\n",
    "\n",
    "optimizer = optim.Adam(params = policy_net.parameters(), lr = lr)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "episode_restart = checkpoint[\"episode\"]\n",
    "tot_steps_done  = checkpoint[\"tot_steps_done\"]\n",
    "\n",
    "target_net   = DQN(em.get_screen_height(), em.get_screen_width(), em.n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()  # since we only use this net for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restart from episode: 250 total steps done: 44449\n"
     ]
    }
   ],
   "source": [
    "print(\"Restart from episode:\", episode_restart, \"total steps done:\", tot_steps_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore arrays of durations, rewards and losses\n",
    "infile_durations = open(filename_durations, 'rb')\n",
    "episode_durations = pickle.load(infile_durations)\n",
    "infile_durations.close()\n",
    "\n",
    "infile_rewards = open(filename_rewards, 'rb')\n",
    "episode_rewards = pickle.load(infile_rewards)\n",
    "infile_rewards.close()\n",
    "\n",
    "infile_losses = open(filename_losses, 'rb')\n",
    "losses = pickle.load(infile_losses)\n",
    "infile_losses.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 250 11105\n"
     ]
    }
   ],
   "source": [
    "print(len(episode_durations), len(episode_rewards), len(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCGPJ028Al1S"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMDrsJNEKuWc",
    "outputId": "52644319-6103-4ee0-e4cd-d679d2f57f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting date and time:  03/08/2021 16:20:17\n"
     ]
    }
   ],
   "source": [
    "# datetime object containing current date and time\n",
    "start = datetime.now()\n",
    "# format: dd/mm/YY H:M:S\n",
    "print(\"Starting date and time: \", start.strftime(\"%d/%m/%Y %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3fynOGbYACLc",
    "outputId": "1635aae3-069f-4103-fdb1-de0753377905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 506\n",
      "Total steps done 90691\n"
     ]
    }
   ],
   "source": [
    "policy_net.train()\n",
    "\n",
    "for episode in range(episode_restart + 1, num_episodes + 1):\n",
    "    em.reset()\n",
    "    state_holder.push(em.get_state())\n",
    "    episode_reward = 0\n",
    "    lives = em.max_lives\n",
    "\n",
    "    for timestep in count():\n",
    "        # Transition handling code\n",
    "        state  = state_holder.get()\n",
    "        action = agent.select_action(tot_steps_done, state, policy_net)\n",
    "        reward, info = em.take_action(action)\n",
    "        episode_reward += reward\n",
    "        life = info['ale.lives'] # or em.env.ale.lives()\n",
    "        \n",
    "        state_holder.push(em.get_state())\n",
    "        next_state = state_holder.get()\n",
    "        \n",
    "        # Trick to significantly improve the convergence of training: if the episode did not end but the\n",
    "        # agent lost a life, then such a transition should be put in ReplayMemory as the final one.\n",
    "        # In this case, it is necessary to continue the episode until done == True\n",
    "        # At the same time, you will teach the agent that losing lives is bad.\n",
    "        \n",
    "        if not em.done:\n",
    "            if life < lives:\n",
    "                next_state, lives = (None, life)\n",
    "        else:\n",
    "            next_state = None\n",
    "            reward = torch.zeros_like(reward) # It has to be a Tensor!\n",
    "        memory.push(state.to('cpu'), action, next_state, reward)\n",
    "        state = next_state\n",
    "        \n",
    "        tot_steps_done += 1\n",
    "        \n",
    "        # Optimization step\n",
    "        if memory.can_provide_sample(batch_size) and tot_steps_done % optimize_model_step == 0:\n",
    "            experiences = memory.sample(batch_size)\n",
    "            \n",
    "            states, actions, next_states, rewards = extract_tensors(experiences)\n",
    "\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(policy_net, target_net, next_states)\n",
    "            # Compute the expected Q values using the Bellman's equation\n",
    "            target_q_values = rewards + (gamma * next_q_values)\n",
    "            \n",
    "            # Compute Huber loss\n",
    "            loss = F.smooth_l1_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            # loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Optimizing the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in policy_net.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Trying to save memory\n",
    "            del experiences\n",
    "            del states\n",
    "            del actions\n",
    "            del next_states\n",
    "            del rewards\n",
    "            del current_q_values\n",
    "            del next_q_values\n",
    "            del target_q_values\n",
    "            del loss\n",
    "\n",
    "        if em.done:\n",
    "            episode_durations.append(timestep)\n",
    "            episode_rewards.append(episode_reward)\n",
    "            print(\"Episode\", episode)\n",
    "            print(\"Total steps done\", tot_steps_done)\n",
    "            if is_ipython: display.clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        if timestep > timestep_max:\n",
    "            break\n",
    "            \n",
    "    if episode % save_fig_step == 0:\n",
    "        plot_durations(episode_durations, 100)\n",
    "        plot_rewards(episode_rewards, 100)\n",
    "        plot_loss(losses)\n",
    "\n",
    "    if episode % target_net_update == 0:\n",
    "        exchange_weights(target_net, policy_net)\n",
    "        save_checkpoint(policy_net, optimizer, episode, tot_steps_done)\n",
    "        save_vectors4plots(episode_durations, episode_rewards, losses)\n",
    "\n",
    "save_weights(policy_net, \"CNN_\" + version)\n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKjudcLDKuWe"
   },
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "# format: dd/mm/YY H:M:S\n",
    "print(f\"Finishing date and time: \", end.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print(f\"Total time training: {end-start}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Breakout_CNN-Copy2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (RL virtualenv)",
   "language": "python",
   "name": "reinforcementl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
